{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natnj/NJ-projects-pub/blob/main/NJ_Homework_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "looking-belize",
      "metadata": {
        "id": "looking-belize"
      },
      "source": [
        "# MLCB Homework 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fatty-banana",
      "metadata": {
        "id": "fatty-banana"
      },
      "source": [
        "**Deadline: Mon Oct 16th 11:59pm**\n",
        "\n",
        "In this homework, you will get some practical experience with modeling small molecules. The goal is to expose you to useful libraries (torch, rdkit, dgl) that are heavily utilized in practice. The questions are designed in a way that you should not need to know these libraries to answer them, but if anything is confusing or if you get stuck please ask for help on Piazza!\n",
        "\n",
        "The homework has 3 parts:\n",
        "\n",
        "1. Introduction to neural networks\n",
        "    1. 1-layer neural network\n",
        "    1. Multi-layer MLP & Activation functions\n",
        "\n",
        "1. Modeling small molecules:\n",
        "    1. Morgan fingerprints & MLP  \n",
        "    1. Smiles strings & RNN  \n",
        "    1. Molecular graphs & GNN  \n",
        "\n",
        "1. Generalization (Required for grad students, optional for undergraduates)\n",
        "\n",
        "1. Competition (Optional)\n",
        "    \n",
        "Answer all questions directly in this notebook and complete the missing code where marked with **COMPLETE HERE**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "experimental-mediterranean",
      "metadata": {
        "id": "experimental-mediterranean"
      },
      "source": [
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inner-phrase",
      "metadata": {
        "id": "inner-phrase"
      },
      "source": [
        "Run the cell below to import all the relevant libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "blind-zimbabwe",
      "metadata": {
        "id": "blind-zimbabwe"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "!pip install torch\n",
        "!pip install rdkit\n",
        "!pip install dgllife\n",
        "!pip install tqdm\n",
        "!pip install  dgl -f https://data.dgl.ai/wheels/repo.html\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_gaussian_quantiles\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from dgllife.data import Tox21\n",
        "from dgllife.utils import SMILESToBigraph, CanonicalAtomFeaturizer, CanonicalBondFeaturizer\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Part 1 datasets\n",
        "X1, Y1 = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_repeated=0, class_sep=5.0, n_classes=2, random_state=1)\n",
        "Y1 = Y1[..., None]\n",
        "X2, Y2 = make_gaussian_quantiles(n_samples=1000, n_features=2, n_classes=2, random_state=1)\n",
        "Y2 = Y2[..., None]\n",
        "\n",
        "# Part 2 datasets\n",
        "smiles_to_g = SMILESToBigraph(\n",
        "    node_featurizer=CanonicalAtomFeaturizer(),\n",
        "    edge_featurizer=CanonicalBondFeaturizer()\n",
        ")\n",
        "Tox21 = Tox21(smiles_to_g)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrapped-image",
      "metadata": {
        "id": "wrapped-image"
      },
      "source": [
        "## 1. Introduction to Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sustainable-proxy",
      "metadata": {
        "id": "sustainable-proxy"
      },
      "source": [
        "In this section, we will briefly review neural networks via a simple Multi-Layer Preceptron (MLP)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "julian-stream",
      "metadata": {
        "id": "julian-stream"
      },
      "source": [
        "### 1.A Single Layer Network (Logistic Regression)\n",
        "\n",
        "First let's consider the simplest model possible, with a single linear layer:\n",
        "\n",
        "$$y = xW + b$$\n",
        "\n",
        "Here, *x* is a feature vector size *n*, which represents the input to the model:\n",
        "\n",
        "$$ x = [x_0, x_1, ... x_n] $$\n",
        "\n",
        "Our goal is to learn the weight matrix *W* and the bias *b* that transform *x* into *y*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chubby-agreement",
      "metadata": {
        "id": "chubby-agreement"
      },
      "source": [
        "#### 1.A.1 Question (1pt)\n",
        "\n",
        "Assume that the input data *x* is 10-dimensional (i.e there are 10 features per sample), and the output *y* is 3-dimensional, what is the total number of parameters learned in the formulation presented above?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "republican-society",
      "metadata": {
        "id": "republican-society"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lucky-intro",
      "metadata": {
        "id": "lucky-intro"
      },
      "source": [
        "#### 1.A.2 Question (2pt)\n",
        "\n",
        "We are now going to train our model from scratch! First we need to define a loss function. We will use the cross entropy loss:\n",
        "\n",
        "$$Loss(y_{pred}, y_{true}) = - (y_{true} \\cdot log(y_{pred}) + (1 - y_{true}) \\cdot log(1 - y_{pred})) $$\n",
        "\n",
        "Here *y_true* is 1 for a positive example and 0 for negatives, and *y_pred* is a probability for the positive class. For simpliciy, we will assume that our input *x* is 2-dimensional. In order to obtain a probability (between 0 and 1) we must normalize the output *y_pred*. We do so using the sigmoid function:\n",
        "\n",
        "$$ y^{pred} = sigmoid(xW + b) = \\frac{e^{xW + b}}{1 + e^{xW + b}}$$\n",
        "\n",
        "In order to learn *W* and *b*, we can use gradient descent: we comptue the derivative of the loss with respect to the parameters.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\frac{dLoss}{dW} &= (y_{pred} - y_{true})x \\\\\n",
        "    \\frac{dLoss}{db} &= (y_{pred} - y_{true})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Implement the functions below! It may be useful to print the shape of the input to make sure you are handling dimensions correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reasonable-enclosure",
      "metadata": {
        "id": "reasonable-enclosure"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation of the input x.\"\"\"\n",
        "    # COMPLETE HERE (hint: use np.exp)\n",
        "    pass\n",
        "\n",
        "def predict(x, W, b):\n",
        "    \"\"\"Returns y_pred given the input and learned parameters.\"\"\"\n",
        "    # COMPLETE HERE (hint: use the sigmoid function above, and np.dot)\n",
        "    pass\n",
        "\n",
        "def loss(y_pred, y_true):\n",
        "    \"\"\"Returns the cross-entropy loss given the prediction and target.\"\"\"\n",
        "    # COMPLETE HERE (hint: use np.log)\n",
        "    # Consider adding a small epsilon to the log if you run into errors.\n",
        "    # Return per sample (no averaging in the first dim)\n",
        "    pass\n",
        "\n",
        "def dLossdW(y_pred, y_true, x):\n",
        "    \"\"\"Comptues the derivative of the loss with respect to W.\"\"\"\n",
        "    # COMPLETE HERE\n",
        "    # Return per sample (no averaging in the first dim)\n",
        "    pass\n",
        "\n",
        "def dLossdb(y_pred, y_true):\n",
        "    \"\"\"Comptues the derivative of the loss with respect to b.\"\"\"\n",
        "    # COMPLETE HERE\n",
        "    # Return per sample (no averaging in the first dim)\n",
        "    pass\n",
        "\n",
        "\n",
        "### DON'T MODIFY BELOW\n",
        "\n",
        "def gradient_descent_solver(x, y_true):\n",
        "    # Initialize weights\n",
        "    W = np.array([0.0, 0.0])[:, None]\n",
        "    b = np.array([0])\n",
        "    alpha = 1.0\n",
        "    num_steps = 1000\n",
        "\n",
        "    # Perform steps of gradient descent\n",
        "    y_pred = predict(x, W, b)\n",
        "    L_start = loss(y_pred, y_true).mean()\n",
        "    accuracy_start = ((y_pred > 0.5) == y_true).mean()\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        y_pred = predict(x, W, b)\n",
        "        L = loss(y_pred, y_true).mean()\n",
        "        accuracy = ((y_pred > 0.5) == y_true).mean()\n",
        "\n",
        "        dW = dLossdW(y_pred, y_true, x)\n",
        "        db = dLossdb(y_pred, y_true)\n",
        "        W = W - alpha * dW.mean(axis=0)[:, None]\n",
        "        b = b - alpha * db.mean(axis=0)\n",
        "\n",
        "    print(\"Start loss: \", L_start)\n",
        "    print(\"Final loss: \", L)\n",
        "\n",
        "    print(\"Start accuracy: \", accuracy_start)\n",
        "    print(\"Final accuracy: \", accuracy)\n",
        "    return W, b\n",
        "\n",
        "\n",
        "def plot_results(x, y_true, W, b):\n",
        "    plt.figure()\n",
        "    plt.scatter(x[:, 0], x[:, 1], c=y_true)\n",
        "\n",
        "    x1 = np.linspace(-10, 10)\n",
        "    x2 = 0 * x1 - 0\n",
        "    plt.plot(x1, x2, c=\"b\", label=\"Starting boundary\")\n",
        "\n",
        "    x1 = np.linspace(-10, 10)\n",
        "    x2 = -W[0] / W[1] * x1 - b / W[1]\n",
        "    plt.plot(x1, x2, c=\"r\", label=\"Final boundary\")\n",
        "\n",
        "    plt.xlim(-10, 10)\n",
        "    plt.ylim(-10, 10)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Run training\n",
        "W1, b1 = gradient_descent_solver(X1, Y1)\n",
        "plot_results(X1, Y1, W1, b1)\n",
        "\n",
        "W2, b2 = gradient_descent_solver(X2, Y2)\n",
        "plot_results(X2, Y2, W2, b2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "saved-month",
      "metadata": {
        "id": "saved-month"
      },
      "source": [
        "**1.A.3 Question (2pt)**\n",
        "\n",
        "Comment on the plots above. How did the model perform on the first dataset? How about the second? Why is the model not appropriate for the second dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "associate-grant",
      "metadata": {
        "id": "associate-grant"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "closed-architect",
      "metadata": {
        "id": "closed-architect"
      },
      "source": [
        "### 1.B Multi-layer Perceptron (MLP)\n",
        "\n",
        "As you found in the example above, some datasets require more complex models to classify correctly. Let's see if we can improve the performance on the second dataset. This time we will implement a 2-layer neural network:\n",
        "\n",
        "$$y = (xW_1 + b_1)W_2 + b_2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ordered-sullivan",
      "metadata": {
        "id": "ordered-sullivan"
      },
      "source": [
        "**1.B.1 Question (1pt)**\n",
        "\n",
        "Prove that the 2-layer model defined above isn't more powerful than a single layer model. (Hint: can you show the model is still just linear?)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verbal-juice",
      "metadata": {
        "id": "verbal-juice"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bizarre-saver",
      "metadata": {
        "id": "bizarre-saver"
      },
      "source": [
        "**1.B.2 Question (3pt)**\n",
        "\n",
        "In order to increase the modeling power of the model, we introduce a \"non-linearity\" between the two layers. Here we choose the simple ReLU function:\n",
        "\n",
        "$$ ReLU(x) = max(0, x) $$\n",
        "\n",
        "We now have the following model:\n",
        "\n",
        "$$y = ReLU(xW_1 + b_1)W_2 + b_2$$\n",
        "\n",
        "The derivative is now more complex so we are going to use the PyTorch library which automates differentiation for us. All we need to do now is define our model and loss function. Fill out the code below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "civil-original",
      "metadata": {
        "scrolled": true,
        "id": "civil-original"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return torch.nn.functional.relu(x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation of the input x.\"\"\"\n",
        "    # COMPLETE HERE (hint: use torch.exp)\n",
        "    pass\n",
        "\n",
        "def predict(x, W1, b1, W2, b2):\n",
        "    \"\"\"Returns y_pred given the input and learned parameters.\"\"\"\n",
        "    # COMPLETE HERE (hint: use the sigmoid & relu functions above + torch.mm)\n",
        "    pass\n",
        "\n",
        "def loss(y_pred, y_true):\n",
        "    \"\"\"Returns the cross-entropy loss given the prediction and target.\"\"\"\n",
        "    # COMPLETE HERE (hint: use torch.log)\n",
        "    # Consider adding a small epsilon to the log if you run into errors.\n",
        "    # Return per sample (no averaging in the first dim)\n",
        "    pass\n",
        "\n",
        "\n",
        "### DON'T MODIFY BELOW\n",
        "\n",
        "def gradient_descent_solver(x, y_true):\n",
        "    # Initialize weights\n",
        "    random = np.random.RandomState(1)\n",
        "    W1 = random.randn(2, 100) * 0.01\n",
        "    W2 = random.randn(100, 1) * 0.01\n",
        "    W1 = torch.nn.Parameter(torch.tensor(W1).float())\n",
        "    b1 = torch.nn.Parameter(torch.zeros((100,)))\n",
        "    W2 = torch.nn.Parameter(torch.tensor(W2).float())\n",
        "    b2 = torch.nn.Parameter(torch.zeros((1,)))\n",
        "    alpha = 0.1\n",
        "    num_steps = 1000\n",
        "\n",
        "    # Perform steps of gradient descent\n",
        "    x = torch.tensor(x).float()\n",
        "    y_true = torch.tensor(y_true).float()\n",
        "    optimizer = torch.optim.SGD([W1, b1, W2, b2], alpha)\n",
        "\n",
        "    y_pred = predict(x, W1, b1, W2, b2)\n",
        "    L_start = loss(y_pred, y_true).mean()\n",
        "    accuracy_start = ((y_pred > 0.5) == y_true).float().mean()\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = predict(x, W1, b1, W2, b2)\n",
        "        L = loss(y_pred, y_true).mean()\n",
        "        L.backward()\n",
        "        optimizer.step()\n",
        "        accuracy = ((y_pred > 0.5) == y_true).float().mean()\n",
        "\n",
        "    print(\"Start loss: \", L_start.item())\n",
        "    print(\"Final loss: \", L.item())\n",
        "\n",
        "    print(\"Start accuracy: \", accuracy_start.item())\n",
        "    print(\"Final accuracy: \", accuracy.item())\n",
        "\n",
        "# Run training\n",
        "print(\"Dataset 1\")\n",
        "gradient_descent_solver(X1, Y1)\n",
        "\n",
        "print(\"\\nDataset 2\")\n",
        "gradient_descent_solver(X2, Y2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vital-commissioner",
      "metadata": {
        "id": "vital-commissioner"
      },
      "source": [
        "**1.B.3 Question (1pt)**\n",
        "\n",
        "Comment on the plot above, how does the model perform now compared to the 1-layer model in the previous problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "italian-output",
      "metadata": {
        "id": "italian-output"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "burning-irish",
      "metadata": {
        "id": "burning-irish"
      },
      "source": [
        "## 2. Modeling small molecules\n",
        "\n",
        "In this problem, you will experiment with various approaches to molecular modeling from the simplest approach to increasingly more complex.\n",
        "\n",
        "For this problem, we consider the Tox21 dataset. Let's visualize some of the molecules in the dataset using RDKit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "secret-limitation",
      "metadata": {
        "id": "secret-limitation"
      },
      "outputs": [],
      "source": [
        "print(Tox21[0][0])\n",
        "Chem.MolFromSmiles(Tox21[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prerequisite-study",
      "metadata": {
        "id": "prerequisite-study"
      },
      "outputs": [],
      "source": [
        "print(Tox21[1][0])\n",
        "Chem.MolFromSmiles(Tox21[1][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "confused-berlin",
      "metadata": {
        "id": "confused-berlin"
      },
      "source": [
        "### 2.A MLP over fingerprints\n",
        "\n",
        "Now we are going to train our first model using a simple MLP over Morgan Fingerprints. Let's look at an example of Morgan fingerprints. You can find more information about it here: https://www.rdkit.org/docs/GettingStartedInPython.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "broken-currency",
      "metadata": {
        "id": "broken-currency"
      },
      "outputs": [],
      "source": [
        "fpgen = AllChem.GetMorganGenerator(radius=3)\n",
        "mol = Chem.MolFromSmiles(Tox21[0][0])\n",
        "ao = AllChem.AdditionalOutput()\n",
        "ao.CollectBitInfoMap()\n",
        "fp = fpgen.GetCountFingerprint(mol, additionalOutput=ao)\n",
        "arr = np.zeros((0,), dtype=np.int8)\n",
        "Chem.DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "print(\"Number of bits: \", len(arr))\n",
        "print(\"Total non zero entries:\", (arr > 0).sum())\n",
        "print(\"Maximum count:\", arr.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "powered-counter",
      "metadata": {
        "id": "powered-counter"
      },
      "source": [
        "The fingerprint is a count vector of length 2048, where each entry corresponds to a substrucuture and the number of times it appears in the molecule."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "turkish-basketball",
      "metadata": {
        "id": "turkish-basketball"
      },
      "source": [
        "**2.A.1 Question (1pt)**\n",
        "\n",
        "Find the most represented bit (maximum count in the array) and visualize it.  Where in the molecule does this pattern appear?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "impossible-priority",
      "metadata": {
        "id": "impossible-priority"
      },
      "outputs": [],
      "source": [
        "def most_represented_bit(arr):\n",
        "    # COMPLETE HERE\n",
        "    pass\n",
        "\n",
        "# Change num_item to any value < count for that bit\n",
        "num_item = 0\n",
        "bi = ao.GetBitInfoMap()\n",
        "idx = most_represented_bit(arr)\n",
        "Chem.Draw.DrawMorganBit(mol, idx, bi, whichExample=num_item)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrapped-emphasis",
      "metadata": {
        "id": "wrapped-emphasis"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ideal-contrast",
      "metadata": {
        "id": "ideal-contrast"
      },
      "source": [
        "**2.A.2 Question (2pt)**\n",
        "\n",
        "We process the dataset so that we have the fingerprints for all molecules, and split the data randomly into training and testing.\n",
        "\n",
        "Note that here we predict 12 labels for each molecule. Because the positive to negative ratio is imbalanced, we measure performance using the ROC-AUC (instead of accuracy) for each label individually and report the average performance across the 12 labels.\n",
        "\n",
        "Implement a small MLP and train it. Try different values for the hidden dimension, fingerprint radius and dropout. Compare results and comment. Try at least 2 other combinations of hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "casual-today",
      "metadata": {
        "id": "casual-today"
      },
      "outputs": [],
      "source": [
        "# COMPLETE HERE (i.e. play with different values)\n",
        "RADIUS = 3\n",
        "HIDDEN_DIM = 128\n",
        "DROPOUT = 0\n",
        "NUM_EPOCHS = 10\n",
        "FP_SIZE = 2048\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.w1 = torch.nn.Linear(in_dim, hidden_dim)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.dropout = torch.nn.Dropout(DROPOUT)\n",
        "        self.w2 = torch.nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        data = batch[\"data\"]\n",
        "        # COMPLETE HERE (hint: each pytorch layer can be called as a function)\n",
        "        pass\n",
        "\n",
        "\n",
        "# DON'T MODIFY BELOW\n",
        "def process_sample(sample):\n",
        "    smiles = sample[0]\n",
        "    labels = sample[2]\n",
        "    mask = sample[3]\n",
        "    fpgen = AllChem.GetMorganGenerator(radius=RADIUS, fpSize=FP_SIZE)\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    ao = AllChem.AdditionalOutput()\n",
        "    ao.CollectBitInfoMap()\n",
        "    fp = fpgen.GetCountFingerprint(mol, additionalOutput=ao)\n",
        "    arr = np.zeros((0,), dtype=np.int8)\n",
        "    Chem.DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "    arr = torch.tensor(arr).float()\n",
        "    return {\"data\": arr, \"labels\": labels, \"mask\": mask}\n",
        "\n",
        "def create_dataset():\n",
        "    dataset = list(map(process_sample, Tox21))\n",
        "    train, test = train_test_split(dataset, test_size=0.2, random_state=1)\n",
        "    return train, test\n",
        "\n",
        "def create_model():\n",
        "    model = MLP(FP_SIZE, HIDDEN_DIM, 12)\n",
        "    return model\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    out_pred, out_labels, out_mask = [], [], []\n",
        "    for batch in dataloader:\n",
        "        mask = batch[\"mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        y_pred = model(batch).sigmoid()\n",
        "        out_pred.append(y_pred)\n",
        "        out_labels.append(labels)\n",
        "        out_mask.append(mask)\n",
        "\n",
        "    out_pred = torch.cat(out_pred).detach().numpy()\n",
        "    out_labels = torch.cat(out_labels).detach().numpy()\n",
        "    out_mask = torch.cat(out_mask).bool().detach().numpy()\n",
        "\n",
        "    aucs = []\n",
        "    for i in range(12):\n",
        "        preds = out_pred[:, i]\n",
        "        labels = out_labels[:, i]\n",
        "        mask = out_mask[:, i]\n",
        "        preds = preds[mask]\n",
        "        labels = labels[mask]\n",
        "        aucs.append(roc_auc_score(labels, preds))\n",
        "\n",
        "    return np.mean(aucs)\n",
        "\n",
        "def train(model, train_dataloader, test_dataloader, num_epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_loss = []\n",
        "    train_aucs = []\n",
        "    test_aucs = []\n",
        "    for _ in tqdm(range(num_epochs), total=num_epochs):\n",
        "        avg_loss = 0\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            mask = batch[\"mask\"]\n",
        "            labels = batch[\"labels\"]\n",
        "            y_pred = model(batch)\n",
        "            loss = torch.nn.functional.binary_cross_entropy_with_logits(y_pred, labels, reduction=\"none\")\n",
        "            loss = (loss * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
        "            loss = loss.mean()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            train_auc = evaluate(model, train_dataloader)\n",
        "            test_auc = evaluate(model, test_dataloader)\n",
        "\n",
        "        avg_loss /= len(train_dataloader)\n",
        "        train_loss.append(avg_loss)\n",
        "        train_aucs.append(train_auc)\n",
        "        test_aucs.append(test_auc)\n",
        "\n",
        "    return train_loss, train_aucs, test_aucs\n",
        "\n",
        "train_set, test_set = create_dataset()\n",
        "train_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "test_dl = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "model = create_model()\n",
        "train_loss, train_aucs, test_aucs = train(model, train_dl, test_dl, num_epochs=NUM_EPOCHS)\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
        "ax1.plot(train_loss)\n",
        "ax1.set_title(\"Training Loss\")\n",
        "ax2.plot(train_aucs)\n",
        "ax2.set_title(\"Training ROC-AUC\")\n",
        "ax3.plot(test_aucs)\n",
        "ax3.set_title(\"Test ROC-AUC\")\n",
        "\n",
        "print(\"Final Training ROC-AUC: \", train_aucs[-1])\n",
        "print(\"Best Training ROC-AUC: \", max(train_aucs))\n",
        "print(\"\\nFinal Testing ROC-AUC: \", test_aucs[-1])\n",
        "print(\"Best Testing ROC-AUC: \", max(test_aucs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "built-saint",
      "metadata": {
        "id": "built-saint"
      },
      "source": [
        "**2.A.3 Question (1pt)**\n",
        "\n",
        "How does the radius affect performance, how about the hidden dimension?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "digital-jimmy",
      "metadata": {
        "id": "digital-jimmy"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "selective-despite",
      "metadata": {
        "id": "selective-despite"
      },
      "source": [
        "**2.A.4 Question (1pt)**\n",
        "\n",
        "Why does the test AUC initially goes up but then starts to fall down? Try increasing the dropout, does it still happen?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "neither-award",
      "metadata": {
        "id": "neither-award"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "happy-sarah",
      "metadata": {
        "id": "happy-sarah"
      },
      "source": [
        "### 2.B RNN over Smiles\n",
        "\n",
        "We now shift our attention to using reccurrent neural network (RNN) over smile strings. First, let's briefly review what an RNN is. RNN's are used to encode sequences. At each step an RNN takes as input it's current state *h* and the input at that step *x*:\n",
        "\n",
        "$$ h_i = f(x_i, h_{i-1}) $$\n",
        "\n",
        "Consider the simplest example possible, where we apply a simple linear layer on the concatenated input and state vectors.\n",
        "\n",
        "$$ h_i = W [x_i, h_{i-1}] + b $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "genetic-administrator",
      "metadata": {
        "id": "genetic-administrator"
      },
      "source": [
        "**2.B.1 Question (1pt)**\n",
        "\n",
        "This formulation leads to exploding / vanishing gradients. Provide some intuition as to why that might be the case. (Hint: consider what happens to W as you start to unroll the computation across multiple steps, what happens to the eigenvalues of W? What does it imply when they are greater than 1, or lesser than 1?)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "religious-notice",
      "metadata": {
        "id": "religious-notice"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stopped-queensland",
      "metadata": {
        "id": "stopped-queensland"
      },
      "source": [
        "In order to address this challenge, and improve RNN's capacity to model long sequences, LSTM's were introduced. Without going in too much detail, LSTMs control the flow of information using learned input and output gates. This has been shown to be an effective way to model longer sequences. You can read more about them here: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "constitutional-infection",
      "metadata": {
        "id": "constitutional-infection"
      },
      "source": [
        "**2.B.2 Question (3pt)**\n",
        "\n",
        "You will now implement an RNN / LSTM. Fill out the missing code below, here again play with some of the values for the hidden dimension and dropout. Comment on your observations. This make take a bit of time to train, so no need to try too many things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "original-testing",
      "metadata": {
        "id": "original-testing"
      },
      "outputs": [],
      "source": [
        "# COMPLETE HERE (i.e. modify to a different value)\n",
        "HIDDEN_DIM = 100\n",
        "DROPOUT = 0\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "class RNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(in_dim, hidden_dim)\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            hidden_dim,\n",
        "            hidden_dim,\n",
        "            dropout=DROPOUT,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.fc = torch.nn.Linear(2 * hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        data = batch[\"data\"]\n",
        "        pad_mask = batch[\"pad_mask\"]\n",
        "        max_len = data.shape[1]\n",
        "\n",
        "        # COMPLETE HERE\n",
        "\n",
        "        # First we embed each input token into a vector\n",
        "        # Use the self.embedding layer\n",
        "        emb = ...\n",
        "\n",
        "        # Compute lengths from padding mask & use the above\n",
        "        lengths = ...\n",
        "\n",
        "        # In order to ignore padding we use\n",
        "        out = torch.nn.utils.rnn.pack_padded_sequence(out, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Now we pass it to the LSTM (which outputs out, state). Ignore the state.\n",
        "        out = ...\n",
        "\n",
        "        # Now we unpack, we use\n",
        "        out = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=max_len)[0]\n",
        "\n",
        "        # Compute the average vector for the sequence\n",
        "        # Beware of padding! Use the mask! Note that you will need to\n",
        "        # expand the mask to a 3D Tensor. You can do so with mask.unsqueeze(-1)\n",
        "        # you will also need to unsqueeze the lengths when dividing by them to take the average\n",
        "        out = ...\n",
        "\n",
        "        # Finally apply the fc layer\n",
        "        out = ...\n",
        "        return out\n",
        "\n",
        "\n",
        "# DON'T MODIFY THIS\n",
        "vocab = {\"~\": 0}\n",
        "def process_sample(sample, max_length):\n",
        "    smiles = sample[0]\n",
        "    labels = sample[2]\n",
        "    mask = sample[3]\n",
        "\n",
        "    tok_ids = []\n",
        "    for token in smiles:\n",
        "        if token not in vocab:\n",
        "            vocab[token] = len(vocab)\n",
        "            tok_id = len(vocab)\n",
        "        else:\n",
        "            tok_id = vocab[token]\n",
        "        tok_ids.append(tok_id)\n",
        "\n",
        "    arr = torch.tensor(tok_ids).long()\n",
        "    return {\"data\": arr, \"labels\": labels, \"mask\": mask}\n",
        "\n",
        "def create_dataset():\n",
        "    max_length = max(len(x[0]) for x in Tox21)\n",
        "    dataset = list(map(lambda x: process_sample(x, max_length), Tox21))\n",
        "    train, test = train_test_split(dataset, test_size=0.2, random_state=1)\n",
        "    return train, test\n",
        "\n",
        "def create_model():\n",
        "    return RNN(len(vocab) + 1, HIDDEN_DIM, 12)\n",
        "\n",
        "def collate_fn(data):\n",
        "\n",
        "    tok_ids = [d[\"data\"] for d in data]\n",
        "    pad_mask = [torch.ones_like(d[\"data\"]) for d in data]\n",
        "    labels = [d[\"labels\"] for d in data]\n",
        "    mask = [d[\"mask\"] for d in data]\n",
        "\n",
        "    tok_ids = torch.nn.utils.rnn.pad_sequence(tok_ids, batch_first=True)\n",
        "    pad_mask = torch.nn.utils.rnn.pad_sequence(pad_mask, batch_first=True)\n",
        "    labels = torch.stack(labels)\n",
        "    mask = torch.stack(mask)\n",
        "\n",
        "    return {\"data\": tok_ids, \"labels\": labels, \"mask\": mask, \"pad_mask\": pad_mask}\n",
        "\n",
        "\n",
        "train_set, test_set = create_dataset()\n",
        "model = create_model()\n",
        "train_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_dl = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "train_loss, train_aucs, test_aucs = train(model, train_dl, test_dl, num_epochs=NUM_EPOCHS)\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
        "ax1.plot(train_loss)\n",
        "ax1.set_title(\"Training Loss\")\n",
        "ax2.plot(train_aucs)\n",
        "ax2.set_title(\"Training ROC-AUC\")\n",
        "ax3.plot(test_aucs)\n",
        "ax3.set_title(\"Test ROC-AUC\")\n",
        "\n",
        "print(\"Final Training ROC-AUC: \", train_aucs[-1])\n",
        "print(\"Best Training ROC-AUC: \", max(train_aucs))\n",
        "print(\"\\nFinal Testing ROC-AUC: \", test_aucs[-1])\n",
        "print(\"Best Testing ROC-AUC: \", max(test_aucs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "opened-lloyd",
      "metadata": {
        "id": "opened-lloyd"
      },
      "source": [
        "**2.B.3 Question (1pt)**\n",
        "\n",
        "How does the performance compare to the MLP baseline? Can you further improve it by modifying the hyperparameters? Try at least 2 other combinations of hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifth-silly",
      "metadata": {
        "id": "fifth-silly"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "norman-circulation",
      "metadata": {
        "id": "norman-circulation"
      },
      "source": [
        "### 2.C GNN over graph\n",
        "\n",
        "Moving on, we will now use the graph representation and a simple message passing neural network (MPNN):\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "m_{ij} &= MLP([h_i, h_j, e_{ij}]) \\\\\n",
        "h_i^{neigh} &= \\sum_j m_{ij} \\\\\n",
        "h_i^{new} &= MLP(h_i, h_i^{neigh})\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "limiting-default",
      "metadata": {
        "id": "limiting-default"
      },
      "source": [
        "**2.C.2 Question (1pt)**\n",
        "\n",
        "For a given molecule, how many message passing steps would you need for every atom to \"see\" every other atom?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "natural-porcelain",
      "metadata": {
        "id": "natural-porcelain"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coated-warehouse",
      "metadata": {
        "id": "coated-warehouse"
      },
      "source": [
        "**2.C.2 Question (3pt)**\n",
        "\n",
        "Again complete the missing code and try a couple of different hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "indian-contribution",
      "metadata": {
        "id": "indian-contribution"
      },
      "outputs": [],
      "source": [
        "import dgl\n",
        "\n",
        "# COMPLETE HERE (i.e. modify to a different value)\n",
        "HIDDEN_DIM = 64\n",
        "NUM_STEPS = 4\n",
        "DROPOUT = 0\n",
        "EPOCHS = 20\n",
        "\n",
        "\n",
        "class GNNLayer(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, dim, dropout):\n",
        "        super().__init__()\n",
        "        self.message_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(dim * 3, dim),\n",
        "            torch.nn.SiLU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(dim, dim)\n",
        "        )\n",
        "        self.node_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(dim * 2, dim),\n",
        "            torch.nn.SiLU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(dim, dim)\n",
        "        )\n",
        "    def message(self, edges):\n",
        "        node_src = edges.src['h']\n",
        "        node_dst = edges.dst['h']\n",
        "        edge = edges.data['e']\n",
        "\n",
        "        # COMPLETE HERE (hint: use torch.cat in dim=-1 to concatenate and apply the message_mlp)\n",
        "        msg = ...\n",
        "        return {'msg_h': msg}\n",
        "\n",
        "    def forward(self, graph, nodes, edges):\n",
        "        with graph.local_scope():\n",
        "            # node feature\n",
        "            graph.ndata['h'] = nodes\n",
        "\n",
        "            # edge feature\n",
        "            graph.edata['e'] = edges\n",
        "\n",
        "            # Compute messages\n",
        "            graph.apply_edges(self.message)\n",
        "            graph.update_all(dgl.function.copy_e('msg_h', 'm'), dgl.function.sum('m', 'h_neigh'))\n",
        "            h_neigh = graph.ndata['h_neigh']\n",
        "\n",
        "            # Compute node updates\n",
        "            # COMPLETE HERE (hint: use torch.cat in dim=-1 to concatenate and apply the node mlp)\n",
        "            h_new = ...\n",
        "            return h_new\n",
        "\n",
        "\n",
        "# DON'T MODIFY THIS\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, dim, dropout):\n",
        "        super().__init__()\n",
        "        self.node_fc = torch.nn.Linear(74, dim)\n",
        "        self.edge_fc = torch.nn.Linear(12, dim)\n",
        "        self.gnn = GNNLayer(dim, dropout)\n",
        "        self.fc = torch.nn.Linear(dim, 12)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        g = batch[\"graph\"]\n",
        "        nodes = self.node_fc(g.ndata[\"h\"])\n",
        "        edges = self.edge_fc(g.edata[\"e\"])\n",
        "\n",
        "        for i in range(NUM_STEPS):\n",
        "            # We add a residual connection with helps with stability\n",
        "            nodes_new = self.gnn(g, nodes, edges)\n",
        "            nodes = nodes + torch.nn.functional.relu(nodes_new)\n",
        "\n",
        "        g.ndata[\"h_out\"] = nodes\n",
        "        out = dgl.mean_nodes(g, \"h_out\")\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "def process_sample(sample):\n",
        "    return {\"graph\": sample[1], \"labels\": sample[2], \"mask\": sample[3]}\n",
        "\n",
        "def create_dataset():\n",
        "    dataset = list(map(process_sample, Tox21))\n",
        "    train, test = train_test_split(dataset, test_size=0.2, random_state=1)\n",
        "    return train, test\n",
        "\n",
        "def create_model():\n",
        "    return GNN(HIDDEN_DIM, DROPOUT)\n",
        "\n",
        "def collate_fn(data):\n",
        "    graph = dgl.batch([d[\"graph\"] for d in data])\n",
        "    labels = torch.stack([d[\"labels\"] for d in data])\n",
        "    mask = torch.stack([d[\"mask\"] for d in data])\n",
        "    return {\"graph\": graph, \"labels\": labels, \"mask\": mask}\n",
        "\n",
        "\n",
        "train_set, test_set = create_dataset()\n",
        "model = create_model()\n",
        "train_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_dl = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "train_loss, train_aucs, test_aucs = train(model, train_dl, test_dl, num_epochs=EPOCHS)\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
        "ax1.plot(train_loss)\n",
        "ax1.set_title(\"Training Loss\")\n",
        "ax2.plot(train_aucs)\n",
        "ax2.set_title(\"Training ROC-AUC\")\n",
        "ax3.plot(test_aucs)\n",
        "ax3.set_title(\"Test ROC-AUC\")\n",
        "\n",
        "print(\"Final Training ROC-AUC: \", train_aucs[-1])\n",
        "print(\"Best Training ROC-AUC: \", max(train_aucs))\n",
        "print(\"\\nFinal Testing ROC-AUC: \", test_aucs[-1])\n",
        "print(\"Best Testing ROC-AUC: \", max(test_aucs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "second-planning",
      "metadata": {
        "id": "second-planning"
      },
      "source": [
        "**2.C.3 Question (1pt)**\n",
        "\n",
        "How does the performance compare to the MLP and RNN baselines? Can you improve it by playing with hyperparameters (you still get full credit regardless)? Try at least 2 other combinations of hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "certified-nickname",
      "metadata": {
        "id": "certified-nickname"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sapphire-walnut",
      "metadata": {
        "id": "sapphire-walnut"
      },
      "source": [
        "### 3. Generalization (Required for grad students, optional for undergaduates)\n",
        "\n",
        "So far, we have assumed random splits of the data. As discussed during lecture, that can be misleading, as performance can vary as we look at molecules that are distant from the training set.\n",
        "\n",
        "Copy paste the code from above and modify it to use a different data split. Do this for all 3 models. You need only modify the *create_dataset* function.\n",
        "\n",
        "Experiment with splitting based on the ScaffoldSplitter.train_val_test_split in dglife:\n",
        "https://lifesci.dgl.ai/api/utils.splitters.html#dgllife.utils.ScaffoldSplitter.train_val_test_split\n",
        "\n",
        "Set frac_val=0 and frac_test=0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bored-foster",
      "metadata": {
        "id": "bored-foster"
      },
      "outputs": [],
      "source": [
        "# Hint: ScaffoldSplitter.train_val_test_split(Tox21, frac_val=0 and frac_test=0.2)\n",
        "from dgllife.utils.splitters import ScaffoldSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comfortable-rogers",
      "metadata": {
        "id": "comfortable-rogers"
      },
      "source": [
        "**3.1 Question (1pt)**\n",
        "\n",
        "How does the performance of the MLP change?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conservative-wilderness",
      "metadata": {
        "id": "conservative-wilderness"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nutritional-graham",
      "metadata": {
        "id": "nutritional-graham"
      },
      "source": [
        "**3.2 Question (1pt)**\n",
        "\n",
        "How does the performance of the LSTM change?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wired-surfing",
      "metadata": {
        "id": "wired-surfing"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "spoken-canberra",
      "metadata": {
        "id": "spoken-canberra"
      },
      "source": [
        "**3.3 Question (1pt)**\n",
        "\n",
        "How does the performance of the GNN change?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "modular-geology",
      "metadata": {
        "id": "modular-geology"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Competition! (Optional to all)\n",
        "\n",
        "Please report the best results you are able to get on the test set on the random split and the scaffold split (if you did part 3). And report the hyperprameters you used!\n",
        "\n",
        "No extra credit, just glory!"
      ],
      "metadata": {
        "id": "X2bY-dM4Lnce"
      },
      "id": "X2bY-dM4Lnce"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER**:"
      ],
      "metadata": {
        "id": "-GpAYlFkMDl7"
      },
      "id": "-GpAYlFkMDl7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}